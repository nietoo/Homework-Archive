{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP HW2 v2.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xea1eP6qcd3E",
        "outputId": "8317e291-e6be-44ab-f8ce-143c74b65b5d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6DRb0Pr7-0V"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import EncoderDecoderModel, EncoderDecoderConfig\n",
        "from transformers import BertModel, BertLMHeadModel, BertConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KihORiLDF0be"
      },
      "source": [
        "RANDOM_SEED=42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "MAX_LENGTH = 41"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaxE75jH8kuY"
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "df_train, df_val = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykRo__H68-CG"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfE8-_GF9yH4"
      },
      "source": [
        "train_data_tokens = tokenizer(df_train['data'].tolist(), padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
        "val_data_tokens = tokenizer(df_val['data'].tolist(), padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
        "test_data_tokens = tokenizer(df_test['data'].tolist(), padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "train_target_tokens = tokenizer(df_train['label'].tolist(), padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
        "val_target_tokens = tokenizer(df_val['label'].tolist(), padding='max_length', truncation=True, max_length=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Q2zbGn-ZM7"
      },
      "source": [
        "class LangDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {'input_ids' : torch.tensor(self.data[idx].ids),\n",
        "                'attention_mask': torch.tensor(self.data[idx].attention_mask),\n",
        "                'labels': torch.tensor(self.targets[idx].ids)\n",
        "        }\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0X-ONaRBCU6"
      },
      "source": [
        "train_dataset = LangDataset(train_data_tokens, train_target_tokens)\n",
        "val_dataset = LangDataset(val_data_tokens, val_target_tokens)\n",
        "test_dataset = LangDataset(test_data_tokens, test_data_tokens)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh7ReLvtHXj0"
      },
      "source": [
        "encoder_config = BertConfig(vocab_size = len(tokenizer),\n",
        "                    max_position_embeddings = MAX_LENGTH + 64, \n",
        "                    num_attention_heads = 6,\n",
        "                    num_hidden_layers = 6)\n",
        "\n",
        "encoder = BertModel(config=encoder_config)\n",
        "\n",
        "\n",
        "decoder_config = BertConfig(vocab_size = len(tokenizer),\n",
        "                    max_position_embeddings = MAX_LENGTH + 64, \n",
        "                    num_attention_heads = 6,\n",
        "                    num_hidden_layers = 6,\n",
        "                    is_decoder=True)  \n",
        "decoder_config.is_decoder = True\n",
        "decoder_config.add_cross_attention = True\n",
        "\n",
        "decoder = BertLMHeadModel(config=decoder_config)\n",
        "\n",
        "\n",
        "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__aRSnpsVJm"
      },
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy='steps',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=10,\n",
        "    save_strategy='steps',\n",
        "    save_steps=400,\n",
        "    eval_accumulation_steps=16\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5OkvYwHsG0Q"
      },
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "448dB7CgsW3Z",
        "outputId": "72173375-d2f5-4a43-834d-deb117c92158"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 985\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 930\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [930/930 22:35, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>11.807700</td>\n",
              "      <td>11.744199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>11.689200</td>\n",
              "      <td>11.535938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>11.480600</td>\n",
              "      <td>11.311883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>11.281700</td>\n",
              "      <td>11.127597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>11.107500</td>\n",
              "      <td>10.951204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>10.942700</td>\n",
              "      <td>10.761515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>10.767100</td>\n",
              "      <td>10.560139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>10.570300</td>\n",
              "      <td>10.354213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>10.370500</td>\n",
              "      <td>10.155472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>10.168100</td>\n",
              "      <td>9.956820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>9.956200</td>\n",
              "      <td>9.755342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>9.758300</td>\n",
              "      <td>9.549658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>9.542800</td>\n",
              "      <td>9.337276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>9.324500</td>\n",
              "      <td>9.119684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>9.092400</td>\n",
              "      <td>8.895988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>8.858900</td>\n",
              "      <td>8.663168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>8.615800</td>\n",
              "      <td>8.421844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>8.383400</td>\n",
              "      <td>8.171880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>8.116800</td>\n",
              "      <td>7.911146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>7.845500</td>\n",
              "      <td>7.640914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>7.574500</td>\n",
              "      <td>7.360004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>7.287800</td>\n",
              "      <td>7.068891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>6.983000</td>\n",
              "      <td>6.769497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>6.678400</td>\n",
              "      <td>6.457078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>6.368500</td>\n",
              "      <td>6.137178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>6.037800</td>\n",
              "      <td>5.804626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>5.696300</td>\n",
              "      <td>5.461998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>5.343700</td>\n",
              "      <td>5.107892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>4.990800</td>\n",
              "      <td>4.745515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.612200</td>\n",
              "      <td>4.372558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>4.242300</td>\n",
              "      <td>3.991576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>3.854400</td>\n",
              "      <td>3.602473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>3.457100</td>\n",
              "      <td>3.208323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>3.067000</td>\n",
              "      <td>2.807951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.662500</td>\n",
              "      <td>2.406272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>2.258500</td>\n",
              "      <td>2.008087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.860000</td>\n",
              "      <td>1.622280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.483800</td>\n",
              "      <td>1.264560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.138300</td>\n",
              "      <td>0.943228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.839700</td>\n",
              "      <td>0.679051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.597400</td>\n",
              "      <td>0.474924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.422800</td>\n",
              "      <td>0.332286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.292000</td>\n",
              "      <td>0.211538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.136716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.118100</td>\n",
              "      <td>0.088770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.079800</td>\n",
              "      <td>0.059477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.055400</td>\n",
              "      <td>0.048454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.038900</td>\n",
              "      <td>0.035261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.031200</td>\n",
              "      <td>0.027948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.022534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.018400</td>\n",
              "      <td>0.017260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.014716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>0.011964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.008100</td>\n",
              "      <td>0.009647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.007900</td>\n",
              "      <td>0.008696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>0.008203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.005400</td>\n",
              "      <td>0.007456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.005200</td>\n",
              "      <td>0.007096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.007122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.006612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>0.006354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.006213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.006163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>0.006078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.005924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.005779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.005660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.005597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.005587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.005557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.005486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.005464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.005429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.005388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.005371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.005365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.005388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.005377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.005339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.005318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.005298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.005252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.005228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.005125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-400\n",
            "Configuration saved in ./results/checkpoint-400/config.json\n",
            "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-800\n",
            "Configuration saved in ./results/checkpoint-800/config.json\n",
            "Model weights saved in ./results/checkpoint-800/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 110\n",
            "  Batch size = 64\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=930, training_loss=3.0981063771832695, metrics={'train_runtime': 1356.6324, 'train_samples_per_second': 21.782, 'train_steps_per_second': 0.686, 'total_flos': 730878880031100.0, 'train_loss': 3.0981063771832695, 'epoch': 30.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MEFa89c7ZcD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUg9Yi9EnbP"
      },
      "source": [
        "batch_size=16\n",
        "\n",
        "id = []\n",
        "data = []\n",
        "label = []\n",
        "\n",
        "for batch_idx in range(len(test_data_tokens.input_ids) // batch_size):\n",
        "    id.extend(df_test.iloc[batch_idx*batch_size : (batch_idx + 1)*batch_size]['id'])\n",
        "    data.extend(df_test.iloc[batch_idx*batch_size : (batch_idx + 1)*batch_size]['data'])\n",
        "    test_data = torch.tensor(test_data_tokens.input_ids[batch_idx*batch_size : (batch_idx + 1)*batch_size])\n",
        "    label.extend(trainer.model_wrapped.generate(test_data.to('cuda'),\n",
        "                                decoder_start_token_id=tokenizer.cls_token_id))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSWGJaPnNWha"
      },
      "source": [
        "labels = [tokenizer.decode(lab, skip_special_tokens=True) for lab in label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_lFVZxZLUlb"
      },
      "source": [
        "df_res = pd.DataFrame({'id': id, 'data': data, 'label': labels})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgQu3AX2Bff7"
      },
      "source": [
        "df_res['label'] = df_res['label'].str.replace(' ', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rHPTHj9OHJw"
      },
      "source": [
        "df_res[['id', 'label']].to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUPi24fiPJY6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "732090ee-4325-4138-9bf3-bf900f9190d7"
      },
      "source": [
        "df_res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>05/10/2007</td>\n",
              "      <td>05-10-2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>elfter september 2007</td>\n",
              "      <td>11-09-2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>09 sa'wol 2077</td>\n",
              "      <td>09-04-2077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>le vingt-huit mai 2077</td>\n",
              "      <td>28-05-2077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>chile gu'wol 2007</td>\n",
              "      <td>07-09-2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4651</th>\n",
              "      <td>4651</td>\n",
              "      <td>осмог јуна 2077</td>\n",
              "      <td>08-06-2077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4652</th>\n",
              "      <td>4652</td>\n",
              "      <td>sipil sibirwol 2077</td>\n",
              "      <td>11-11-2077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4653</th>\n",
              "      <td>4653</td>\n",
              "      <td>yuke i'wol 2049</td>\n",
              "      <td>06-02-2049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4654</th>\n",
              "      <td>4654</td>\n",
              "      <td>четырнадцатого 07 2049</td>\n",
              "      <td>14-07-2049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4655</th>\n",
              "      <td>4655</td>\n",
              "      <td>18 samwol 2007</td>\n",
              "      <td>18-03-2007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4656 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                    data       label\n",
              "0        0              05/10/2007  05-10-2007\n",
              "1        1   elfter september 2007  11-09-2007\n",
              "2        2          09 sa'wol 2077  09-04-2077\n",
              "3        3  le vingt-huit mai 2077  28-05-2077\n",
              "4        4       chile gu'wol 2007  07-09-2007\n",
              "...    ...                     ...         ...\n",
              "4651  4651         осмог јуна 2077  08-06-2077\n",
              "4652  4652     sipil sibirwol 2077  11-11-2077\n",
              "4653  4653         yuke i'wol 2049  06-02-2049\n",
              "4654  4654  четырнадцатого 07 2049  14-07-2049\n",
              "4655  4655          18 samwol 2007  18-03-2007\n",
              "\n",
              "[4656 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mZNork-HQjQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}